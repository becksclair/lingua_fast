[package]
name     = "lingua-fast"
version  = "0.1.0"
edition  = "2021"
authors  = ["Becks & Sam"]
license  = "MIT"
resolver = "2"


[dependencies]
axum               = { version = "0.7", features = ["macros"] }
tokio              = { version = "1", features = ["rt-multi-thread", "macros", "signal"] }
serde              = { version = "1", features = ["derive"] }
serde_json         = "1"
thiserror          = "1"
tracing            = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "fmt"] }
anyhow             = "1"
bytes              = "1"
hyper              = { version = "1", features = ["http1", "http2", "server"] }
http               = "1"
encoding_rs        = "0.8"
num_cpus           = "1"
# JSON Schema validation
jsonschema = { version = "0.18", default-features = false, features = [
    "draft202012",
] }
# async trait for backend abstraction
async-trait = "0.1"
# llama.cpp Rust bindings (optional; enable with feature `llama`)
# We use the high-level safe wrappers from `llama-cpp-2` to keep wiring simple.
llama-cpp-2 = { version = "0.1.121", optional = true, default-features = false }
# time & metrics
hdrhistogram                = "7"
metrics                     = "0.22"
metrics-exporter-prometheus = "0.14"
parking_lot                 = "0.12"
clap                        = { version = "4", features = ["derive", "env"] }
dotenvy                     = "0.15"
once_cell                   = "1"


[dev-dependencies]
walkdir = "^2"
reqwest = { version = "0.12", features = ["json", "http2", "gzip"] }
rand    = "0.8"
tower   = { version = "0.5", features = ["util"] }


[build-dependencies]
cc = "1"


[workspace]
members = ["xtask"]

[features]
# Always use real llama.cpp backend
default = ["llama"]
llama = ["dep:llama-cpp-2"]

[profile.release]
codegen-units = 1
lto           = true
opt-level     = 3
strip         = true
panic         = "abort"
