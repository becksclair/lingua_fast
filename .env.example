MODEL_PATH=/path/to/granite-3.3-2b-instruct-Q4_K_M.gguf
BIND_ADDR=0.0.0.0:8080

# CPU threads for llama.cpp; 0 = auto
# On M2 16GB, 4â€“6 often performs best
THREADS=4

# Per-process inference concurrency; 0 = auto (min(8, num_cpus))
# On M2 + Metal, 1 is recommended to avoid GPU thrash
INFER_CONCURRENCY=1

# Generation limits and sampling
MAX_TOKENS=768
TEMP=0.35
TOP_P=0.9
MIN_P=0.05
REPEAT_PENALTY=1.1

# Context window and batching
N_CTX=2048
N_BATCH=1024

# GPU offload: set high to offload all layers supported by Metal
N_GPU_LAYERS=999
